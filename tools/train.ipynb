{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.dataset as mydataset\n",
    "import models.models as mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_batchsize =  4\n",
    "arg_workers = 8\n",
    "arg_epochs = 50\n",
    "arg_lr = 2e-4\n",
    "arg_b1 = 0.5\n",
    "arg_b2 = 0.999\n",
    "arg_layer_depth = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "# Define Generator\n",
    "# monet -> photo\n",
    "netG_A = mymodel.Generator(3, arg_layer_depth).to(device)\n",
    "# photo -> monet\n",
    "netG_B = mymodel.Generator(3, arg_layer_depth).to(device)\n",
    "\n",
    "# Define Discrimator\n",
    "netD_A = mymodel.Discriminator(3, arg_layer_depth).to(device)\n",
    "netD_B = mymodel.Discriminator(3, arg_layer_depth).to(device)\n",
    "\n",
    "# weight initialization\n",
    "mymodel.weight_init(netG_A)\n",
    "mymodel.weight_init(netG_B)\n",
    "mymodel.weight_init(netD_A)\n",
    "mymodel.weight_init(netD_B)\n",
    "\n",
    "\n",
    "# optimizer\n",
    "netG_optim = optim.Adam(itertools.chain(netG_A.parameters(), netG_B.parameters()), lr = arg_lr, betas = (arg_b1, arg_b2))\n",
    "netD_optim = optim.Adam(itertools.chain(netD_A.parameters(), netD_B.parameters()), lr = arg_lr, betas = (arg_b1, arg_b2))\n",
    "\n",
    "GAN_LOSS = nn.MSELoss()\n",
    "Cycle_LOSS = nn.L1Loss()\n",
    "Identity_LOSS = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paint_dataset = mydataset.Paint_Dataset(\"../data/\", 256, arg_workers, arg_batchsize)\n",
    "random_photo_dataset = mydataset.Random_Photo_Dataset(\"../data/\", 256, arg_batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "    Parameters:\n",
    "        nets (network list)   -- a list of networks\n",
    "        requires_grad (bool)  -- whether the networks require gradients or not\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = {}\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['D_losses'] = []\n",
    "\n",
    "print('train is starting')\n",
    "\n",
    "for epoch in range(arg_epochs):\n",
    "    t = time.time()\n",
    "    \n",
    "    netG_A.train()\n",
    "    netG_B.train()\n",
    "    \n",
    "    netD_A.train()\n",
    "    netD_B.train()\n",
    "    \n",
    "    G_losses = 0\n",
    "    D_losses = 0\n",
    "    \n",
    "    paint_dataloader = paint_dataset.get_dataloader()\n",
    "\n",
    "    for A in paint_dataloader:\n",
    "        B = random_photo_dataset.getRandomBatch()\n",
    "        \n",
    "        A = A[0].to(device)\n",
    "        B = B.to(device)\n",
    "        \n",
    "        A2B = netG_A(A)\n",
    "        B2A = netG_B(B)\n",
    "\n",
    "        A2B2A = netG_B(A2B)\n",
    "        B2A2B = netG_A(B2A)\n",
    "\n",
    "        set_requires_grad([netD_A, netD_B], False)\n",
    "\n",
    "        pred_fake_A = netD_A(B2A)\n",
    "        pred_fake_B = netD_B(A2B)\n",
    "\n",
    "        G_GAN_loss = GAN_LOSS(pred_fake_A, torch.ones_like(pred_fake_A)) +\\\n",
    "                        GAN_LOSS(pred_fake_B, torch.ones_like(pred_fake_B))\n",
    "        G_Cycle_loss = Cycle_LOSS(A2B2A, A) + Cycle_LOSS(B2A2B, B)\n",
    "        G_identity_loss = Identity_LOSS(netG_A(B), B) + Identity_LOSS(netG_B(A), A)\n",
    "        G_loss = G_GAN_loss + 10 * G_Cycle_loss + 5 * G_identity_loss\n",
    "\n",
    "        netG_optim.zero_grad()\n",
    "        G_loss.backward()\n",
    "        netG_optim.step()\n",
    "        \n",
    "        set_requires_grad([netD_A, netD_B], True)\n",
    "\n",
    "        pred_real_A = netD_A(A)\n",
    "        pred_fake_A = netD_A(B2A.detach())\n",
    "\n",
    "        D_A_loss = GAN_LOSS(pred_real_A, torch.ones_like(pred_real_A)) +\\\n",
    "                    GAN_LOSS(pred_fake_A, torch.zeros_like(pred_fake_A))\n",
    "\n",
    "        pred_real_B = netD_B(B)\n",
    "        pred_fake_B = netD_B(A2B.detach())\n",
    "\n",
    "        \n",
    "        D_B_loss = GAN_LOSS(pred_real_B, torch.ones_like(pred_real_B)) +\\\n",
    "                    GAN_LOSS(pred_fake_B, torch.zeros_like(pred_fake_B))\n",
    "        \n",
    "        D_loss = (D_A_loss + D_B_loss) / 2\n",
    "\n",
    "        netD_optim.zero_grad()\n",
    "        D_loss.backward()\n",
    "        netD_optim.step()\n",
    "\n",
    "        D_losses += D_loss.item() / len(paint_dataloader)\n",
    "        G_losses += G_loss.item() / len(paint_dataloader)\n",
    "\n",
    "    print(f'[{epoch + 1}/{arg_epochs}]\\tD_loss : {D_losses:.6f}\\tG_loss : {G_losses:.6f}\\ttime : {time.time() - t:.3f}s')\n",
    "    \n",
    "    train_hist['G_losses'].append(G_losses)\n",
    "    train_hist['D_losses'].append(D_losses)\n",
    "\n",
    "    # save model per 10epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        if not os.path.exists('../models/model_G'):\n",
    "            os.makedirs('../models/model_G')\n",
    "\n",
    "        if not os.path.exists('../models/model_D'):\n",
    "            os.makedirs('../models/model_D')\n",
    "\n",
    "        torch.save(netG_A.state_dict, '../models/model_G/' + f'netG(uNet)_A{epoch + 1}.pt')\n",
    "        torch.save(netG_B.state_dict, '../models/model_G/' + f'netG(uNet)_B{epoch + 1}.pt')\n",
    "\n",
    "        torch.save(netD_A.state_dict, '../models/model_D/' + f'netD(uNet)_A{epoch + 1}.pt')\n",
    "        torch.save(netD_B.state_dict, '../models/model_D/' + f'netD(uNet)_B{epoch + 1}.pt')\n",
    "\n",
    "        print(f'Model is saved at {epoch + 1}epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG_A.state_dict, '../models/model_G/final_model_G_A.pt')\n",
    "torch.save(netG_B.state_dict, '../models/model_G/final_model_G_B.pt')\n",
    "torch.save(netD_A.state_dict, '../models/model_D/final_model_D_A.pt')\n",
    "torch.save(netD_B.state_dict, '../models/model_D/final_model_D_B.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c6b1850e54510a708682478a6a7fb41a5eeb954c216c5ab9399aec50c6996c1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
